Phase 3: CCD-Aware Spreading Proof
Your EKS nodes are currently running with:

CPU Manager Policy: static (Cores are pinned, confirmed by successful EKS config update).

Scheduler: Default (Cores are being packed sequentially within the node).

Workload: 10 Sysbench pods are currently running, packed sequentially across the pinned cores.

Our next action is to introduce the CCD-awareness and the spreading logic.

Step 3.1: Deploy Node Feature Discovery (NFD)
We deploy the tool that discovers the physical CCDs (L3 Cache partitions on your AMD EPYC CPUs) and applies custom labels to the nodes.

Deploy NFD Labeller:

Bash

kubectl apply -f ccd-labeller.yaml
(Assuming this manifest correctly deploys a DaemonSet that can detect CCD topology).

Verify New Labels (The CCD Domain Proof): Wait about 30 seconds for the NFD pods to start and apply the labels. Then, inspect Node A.

Bash

NODE_A=$(kubectl get nodes --no-headers | awk 'NR==1{print $1}')
kubectl get node $NODE_A --show-labels | grep ccd-id
Expected Proof Output: You must see custom labels. Since you have a large m7a.12xlarge (48 cores), you will likely see 6, 8, or more distinct CCD domains.

# Example Expected Output:
custom.io/ccd-id.ccd-0=true
custom.io/ccd-id.ccd-1=true
custom.io/ccd-id.ccd-2=true
...
Step 3.2: Deploy with Topology Spread (The Final Proof of Spreading)
Now we deploy the final workload that tells the scheduler to use these new labels to override its default packing logic.

Clean Up Old Deployment (Removes Packing Logic Pods):

Bash

kubectl delete deployment sysbench-default
Deploy the CCD-Aware Workload:

You need to create and apply the sysbench-ccd-aware.yaml (which uses the topologySpreadConstraints feature).

YAML

# Example content of sysbench-ccd-aware.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sysbench-ccd-aware
spec:
  replicas: 10 # 10 Pods * 4 cores each
  selector:
    matchLabels:
      app: my-ccd-sensitive-app
  template:
    metadata:
      labels:
        app: my-ccd-sensitive-app
    spec:
      containers:
      - name: sysbench-runner
        image: ubuntu:latest
        # ... (Resource Requests/Limits: cpu: "4" for pinning)

      # ðŸ‘‡ THIS IS THE SCHEDULER OVERRIDE LOGIC ðŸ‘‡
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: custom.io/ccd-id  # Uses the label created by NFD
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: my-ccd-sensitive-app
Bash

kubectl apply -f sysbench-ccd-aware.yaml
Final Proof Check: Observe the placement of the 10 pods.

Bash

kubectl get pods -l app=my-ccd-sensitive-app -o wide
